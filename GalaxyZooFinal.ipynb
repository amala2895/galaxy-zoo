{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Galaxy Zoo main notebook\n",
    "\n",
    "### Python files required to run ths notebook: data_loader.py, YLabelCreate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1067ccb10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='Galaxy zoo project')\n",
    "parser.add_argument('--data', type=str, default='data', metavar='D',\n",
    "                    help=\"folder where data is located. train_data.zip and test_data.zip need to be found in the folder\")\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--epochs', type=int, default=30, metavar='N',\n",
    "                    help='number of epochs to train (default: 15)')\n",
    "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                    help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "\n",
    "\n",
    "input_args = \"\"\n",
    "args = parser.parse_args(input_args)\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Initialization and Loading\n",
    "from data_loader import initialize_data, loader#, data_transforms # data.py in the same folder\n",
    "initialize_data(args.data) # extracts the zip files, makes a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_of_training_points = 5000\n",
    "number_of_validation_points = 1000\n",
    "\n",
    "from YLabelCreate import getYlabel\n",
    "\n",
    "label_ids_training, label_ids_validation, label_values_training, label_values_validation = getYlabel(number_of_training_points, number_of_validation_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crop_size = 256\n",
    "resolution = 64\n",
    "batch_size = 64\n",
    "shuffle = False\n",
    "questions = 1\n",
    "\n",
    "train_loader = loader(label_ids_training, label_values_training, crop_size, resolution, batch_size, shuffle, questions)\n",
    "\n",
    "validation_loader = loader(label_ids_validation, label_values_validation, crop_size, resolution, batch_size, shuffle, questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_loader)\n",
    "type(validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 64, 64])\n",
      "torch.Size([64, 3])\n"
     ]
    }
   ],
   "source": [
    "for ix, (data,target) in enumerate(train_loader):\n",
    "    print(data.shape)\n",
    "    print(target.shape)\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Neural Network and Optimizer\n",
    "# We define neural net in model.py so that it can be reused by the evaluate.py script\n",
    "from question_wise_model import Net\n",
    "model = Net(questions)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "loss_train = nn.MSELoss()\n",
    "loss_validation = nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target).float()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_train(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validation():\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in validation_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target).float()\n",
    "        output = model(data)\n",
    "        validation_loss += loss_validation(output, target) # sum up batch loss\n",
    "        #pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        #correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    validation_loss /= len(validation_loader.dataset)\n",
    "    print('\\nValidation set: Average loss:' +  str(validation_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amaladeshpande/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/5000 (0%)]\tLoss: 0.178720\n",
      "Train Epoch: 1 [640/5000 (13%)]\tLoss: 7.776305\n",
      "Train Epoch: 1 [1280/5000 (25%)]\tLoss: 0.221660\n",
      "Train Epoch: 1 [1920/5000 (38%)]\tLoss: 0.217741\n",
      "Train Epoch: 1 [2560/5000 (51%)]\tLoss: 0.203061\n",
      "Train Epoch: 1 [3200/5000 (63%)]\tLoss: 0.208899\n",
      "Train Epoch: 1 [3840/5000 (76%)]\tLoss: 0.221185\n",
      "Train Epoch: 1 [4480/5000 (89%)]\tLoss: 0.219094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amaladeshpande/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation set: Average loss:tensor(0.6485, grad_fn=<DivBackward0>)\n",
      "Train Epoch: 2 [0/5000 (0%)]\tLoss: 0.211171\n",
      "Train Epoch: 2 [640/5000 (13%)]\tLoss: 0.212079\n",
      "Train Epoch: 2 [1280/5000 (25%)]\tLoss: 0.221660\n",
      "Train Epoch: 2 [1920/5000 (38%)]\tLoss: 0.296018\n",
      "Train Epoch: 2 [2560/5000 (51%)]\tLoss: 0.203061\n",
      "Train Epoch: 2 [3200/5000 (63%)]\tLoss: 0.208899\n",
      "Train Epoch: 2 [3840/5000 (76%)]\tLoss: 0.221185\n",
      "Train Epoch: 2 [4480/5000 (89%)]\tLoss: 0.219094\n",
      "\n",
      "Validation set: Average loss:tensor(0.6485, grad_fn=<DivBackward0>)\n",
      "Train Epoch: 3 [0/5000 (0%)]\tLoss: 0.211171\n",
      "Train Epoch: 3 [640/5000 (13%)]\tLoss: 0.212079\n",
      "Train Epoch: 3 [1280/5000 (25%)]\tLoss: 0.221660\n",
      "Train Epoch: 3 [1920/5000 (38%)]\tLoss: 0.217741\n",
      "Train Epoch: 3 [2560/5000 (51%)]\tLoss: 0.203061\n",
      "Train Epoch: 3 [3200/5000 (63%)]\tLoss: 0.208899\n",
      "Train Epoch: 3 [3840/5000 (76%)]\tLoss: 0.221185\n",
      "Train Epoch: 3 [4480/5000 (89%)]\tLoss: 0.219094\n",
      "\n",
      "Validation set: Average loss:tensor(0.6485, grad_fn=<DivBackward0>)\n",
      "Train Epoch: 4 [0/5000 (0%)]\tLoss: 0.211171\n",
      "Train Epoch: 4 [640/5000 (13%)]\tLoss: 0.212079\n",
      "Train Epoch: 4 [1280/5000 (25%)]\tLoss: 0.221660\n",
      "Train Epoch: 4 [1920/5000 (38%)]\tLoss: 0.217741\n",
      "Train Epoch: 4 [2560/5000 (51%)]\tLoss: 0.203061\n",
      "Train Epoch: 4 [3200/5000 (63%)]\tLoss: 0.208899\n",
      "Train Epoch: 4 [3840/5000 (76%)]\tLoss: 0.221185\n",
      "Train Epoch: 4 [4480/5000 (89%)]\tLoss: 0.219094\n",
      "\n",
      "Validation set: Average loss:tensor(0.6485, grad_fn=<DivBackward0>)\n",
      "Train Epoch: 5 [0/5000 (0%)]\tLoss: 0.211171\n",
      "Train Epoch: 5 [640/5000 (13%)]\tLoss: 0.212079\n",
      "Train Epoch: 5 [1280/5000 (25%)]\tLoss: 0.221660\n",
      "Train Epoch: 5 [1920/5000 (38%)]\tLoss: 0.217741\n",
      "Train Epoch: 5 [2560/5000 (51%)]\tLoss: 0.203061\n",
      "Train Epoch: 5 [3200/5000 (63%)]\tLoss: 0.208899\n",
      "Train Epoch: 5 [3840/5000 (76%)]\tLoss: 0.221185\n",
      "Train Epoch: 5 [4480/5000 (89%)]\tLoss: 0.219094\n",
      "\n",
      "Validation set: Average loss:tensor(0.6485, grad_fn=<DivBackward0>)\n",
      "Train Epoch: 6 [0/5000 (0%)]\tLoss: 0.211171\n",
      "Train Epoch: 6 [640/5000 (13%)]\tLoss: 0.212079\n",
      "Train Epoch: 6 [1280/5000 (25%)]\tLoss: 0.221660\n",
      "Train Epoch: 6 [1920/5000 (38%)]\tLoss: 0.217741\n",
      "Train Epoch: 6 [2560/5000 (51%)]\tLoss: 0.203061\n",
      "Train Epoch: 6 [3200/5000 (63%)]\tLoss: 0.208899\n",
      "Train Epoch: 6 [3840/5000 (76%)]\tLoss: 0.221185\n",
      "Train Epoch: 6 [4480/5000 (89%)]\tLoss: 0.219094\n",
      "\n",
      "Validation set: Average loss:tensor(0.6485, grad_fn=<DivBackward0>)\n",
      "Train Epoch: 7 [0/5000 (0%)]\tLoss: 0.211171\n",
      "Train Epoch: 7 [640/5000 (13%)]\tLoss: 0.212079\n",
      "Train Epoch: 7 [1280/5000 (25%)]\tLoss: 0.221660\n",
      "Train Epoch: 7 [1920/5000 (38%)]\tLoss: 0.217741\n",
      "Train Epoch: 7 [2560/5000 (51%)]\tLoss: 0.203061\n",
      "Train Epoch: 7 [3200/5000 (63%)]\tLoss: 0.208899\n",
      "Train Epoch: 7 [3840/5000 (76%)]\tLoss: 0.221185\n",
      "Train Epoch: 7 [4480/5000 (89%)]\tLoss: 0.219094\n",
      "\n",
      "Validation set: Average loss:tensor(0.6485, grad_fn=<DivBackward0>)\n",
      "Train Epoch: 8 [0/5000 (0%)]\tLoss: 0.211171\n",
      "Train Epoch: 8 [640/5000 (13%)]\tLoss: 0.212079\n",
      "Train Epoch: 8 [1280/5000 (25%)]\tLoss: 0.221660\n",
      "Train Epoch: 8 [1920/5000 (38%)]\tLoss: 0.217741\n",
      "Train Epoch: 8 [2560/5000 (51%)]\tLoss: 0.203061\n",
      "Train Epoch: 8 [3200/5000 (63%)]\tLoss: 0.208899\n",
      "Train Epoch: 8 [3840/5000 (76%)]\tLoss: 0.221185\n",
      "Train Epoch: 8 [4480/5000 (89%)]\tLoss: 0.219094\n",
      "\n",
      "Validation set: Average loss:tensor(0.6485, grad_fn=<DivBackward0>)\n",
      "Train Epoch: 9 [0/5000 (0%)]\tLoss: 0.211171\n",
      "Train Epoch: 9 [640/5000 (13%)]\tLoss: 0.212079\n",
      "Train Epoch: 9 [1280/5000 (25%)]\tLoss: 0.221660\n",
      "Train Epoch: 9 [1920/5000 (38%)]\tLoss: 0.217741\n",
      "Train Epoch: 9 [2560/5000 (51%)]\tLoss: 0.203061\n",
      "Train Epoch: 9 [3200/5000 (63%)]\tLoss: 0.208899\n",
      "Train Epoch: 9 [3840/5000 (76%)]\tLoss: 0.221185\n",
      "Train Epoch: 9 [4480/5000 (89%)]\tLoss: 0.219094\n",
      "\n",
      "Validation set: Average loss:tensor(0.6485, grad_fn=<DivBackward0>)\n",
      "Train Epoch: 10 [0/5000 (0%)]\tLoss: 0.211171\n",
      "Train Epoch: 10 [640/5000 (13%)]\tLoss: 0.212079\n",
      "Train Epoch: 10 [1280/5000 (25%)]\tLoss: 0.221660\n",
      "Train Epoch: 10 [1920/5000 (38%)]\tLoss: 0.217741\n",
      "Train Epoch: 10 [2560/5000 (51%)]\tLoss: 0.203061\n",
      "Train Epoch: 10 [3200/5000 (63%)]\tLoss: 0.208899\n",
      "Train Epoch: 10 [3840/5000 (76%)]\tLoss: 0.221185\n",
      "Train Epoch: 10 [4480/5000 (89%)]\tLoss: 0.219094\n",
      "\n",
      "Validation set: Average loss:tensor(0.6485, grad_fn=<DivBackward0>)\n",
      "Train Epoch: 11 [0/5000 (0%)]\tLoss: 0.211171\n",
      "Train Epoch: 11 [640/5000 (13%)]\tLoss: 0.212079\n",
      "Train Epoch: 11 [1280/5000 (25%)]\tLoss: 0.221660\n",
      "Train Epoch: 11 [1920/5000 (38%)]\tLoss: 0.217741\n",
      "Train Epoch: 11 [2560/5000 (51%)]\tLoss: 0.203061\n",
      "Train Epoch: 11 [3200/5000 (63%)]\tLoss: 0.208899\n",
      "Train Epoch: 11 [3840/5000 (76%)]\tLoss: 0.221185\n",
      "Train Epoch: 11 [4480/5000 (89%)]\tLoss: 0.219094\n",
      "\n",
      "Validation set: Average loss:tensor(0.6485, grad_fn=<DivBackward0>)\n",
      "Train Epoch: 12 [0/5000 (0%)]\tLoss: 0.211171\n",
      "Train Epoch: 12 [640/5000 (13%)]\tLoss: 0.212079\n",
      "Train Epoch: 12 [1280/5000 (25%)]\tLoss: 0.221660\n",
      "Train Epoch: 12 [1920/5000 (38%)]\tLoss: 0.217741\n",
      "Train Epoch: 12 [2560/5000 (51%)]\tLoss: 0.203061\n",
      "Train Epoch: 12 [3200/5000 (63%)]\tLoss: 0.208899\n",
      "Train Epoch: 12 [3840/5000 (76%)]\tLoss: 0.221185\n",
      "Train Epoch: 12 [4480/5000 (89%)]\tLoss: 0.219094\n",
      "\n",
      "Validation set: Average loss:tensor(0.6485, grad_fn=<DivBackward0>)\n",
      "Train Epoch: 13 [0/5000 (0%)]\tLoss: 0.211171\n",
      "Train Epoch: 13 [640/5000 (13%)]\tLoss: 0.212079\n",
      "Train Epoch: 13 [1280/5000 (25%)]\tLoss: 0.221660\n",
      "Train Epoch: 13 [1920/5000 (38%)]\tLoss: 0.217741\n",
      "Train Epoch: 13 [2560/5000 (51%)]\tLoss: 0.203061\n",
      "Train Epoch: 13 [3200/5000 (63%)]\tLoss: 0.208899\n",
      "Train Epoch: 13 [3840/5000 (76%)]\tLoss: 0.221185\n",
      "Train Epoch: 13 [4480/5000 (89%)]\tLoss: 0.219094\n",
      "\n",
      "Validation set: Average loss:tensor(0.6485, grad_fn=<DivBackward0>)\n",
      "Train Epoch: 14 [0/5000 (0%)]\tLoss: 0.211171\n",
      "Train Epoch: 14 [640/5000 (13%)]\tLoss: 0.212079\n",
      "Train Epoch: 14 [1280/5000 (25%)]\tLoss: 0.221660\n",
      "Train Epoch: 14 [1920/5000 (38%)]\tLoss: 0.217741\n",
      "Train Epoch: 14 [2560/5000 (51%)]\tLoss: 0.203061\n",
      "Train Epoch: 14 [3200/5000 (63%)]\tLoss: 0.208899\n",
      "Train Epoch: 14 [3840/5000 (76%)]\tLoss: 0.221185\n",
      "Train Epoch: 14 [4480/5000 (89%)]\tLoss: 0.219094\n",
      "\n",
      "Validation set: Average loss:tensor(0.6485, grad_fn=<DivBackward0>)\n",
      "Train Epoch: 15 [0/5000 (0%)]\tLoss: 0.211171\n",
      "Train Epoch: 15 [640/5000 (13%)]\tLoss: 0.212079\n",
      "Train Epoch: 15 [1280/5000 (25%)]\tLoss: 0.221660\n",
      "Train Epoch: 15 [1920/5000 (38%)]\tLoss: 0.217741\n",
      "Train Epoch: 15 [2560/5000 (51%)]\tLoss: 0.203061\n",
      "Train Epoch: 15 [3200/5000 (63%)]\tLoss: 0.208899\n",
      "Train Epoch: 15 [3840/5000 (76%)]\tLoss: 0.221185\n",
      "Train Epoch: 15 [4480/5000 (89%)]\tLoss: 0.219094\n",
      "\n",
      "Validation set: Average loss:tensor(0.6485, grad_fn=<DivBackward0>)\n",
      "Train Epoch: 16 [0/5000 (0%)]\tLoss: 0.211171\n",
      "Train Epoch: 16 [640/5000 (13%)]\tLoss: 0.212079\n",
      "Train Epoch: 16 [1280/5000 (25%)]\tLoss: 0.221660\n",
      "Train Epoch: 16 [1920/5000 (38%)]\tLoss: 0.217741\n",
      "Train Epoch: 16 [2560/5000 (51%)]\tLoss: 0.203061\n",
      "Train Epoch: 16 [3200/5000 (63%)]\tLoss: 0.208899\n",
      "Train Epoch: 16 [3840/5000 (76%)]\tLoss: 0.221185\n",
      "Train Epoch: 16 [4480/5000 (89%)]\tLoss: 0.219094\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1ae879ef44f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'model_question_wise.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-c1c529804a74>\u001b[0m in \u001b[0;36mvalidation\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mvalidation_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# sum up batch loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m#pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/galaxy-zoo/question_wise_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m#x = F.max_pool2d(self.bn3(self.conv3_drop(F.relu(self.conv3(x)))), 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1252\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1253\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m     )\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(epoch)\n",
    "    validation()\n",
    "    model_file = 'model_question_wise.pth'\n",
    "    torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
