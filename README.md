# galaxy-zoo

This project aims at predicting probabilities of answers to questions asked in Galaxy Zoo crowdsourcing project for each Galaxy Image. 

We have run this project on NYU Prince server using Slurm batch script. 

Following are the batch scripts used - 
1. run_augmented.s - this batch script is used to run the model on augmented data
2. run_questionwise.s - this batch script is used to run the model question wise independently on augmented data

To run this project anaconda3/5.3.1 needs to be installed and further requiremnets are listed in requirements.yaml

To create the virtual environment use the command - conda env create -f requirements.yaml

Further the environment is activated using command - source activate vision

# Galaxy Zoo Data
The Galaxy Zoo data consists of images of galaxies and the probabilities of the answers of the questions asked to the public. There are total 11 questions and each having different set of answers(total of 37 answers). The questions are dependent on each other, i.e. depending on the answer of previous question a new question is asked. This is reflected in the probabilities as they are weighted accordingly. 

question_map.py - This file contains mapping of weighting of probablities as well as question to answers mapping. 

# Data Loading

The Y label for this project is the 37 length vector of answers which is given in file training_solutions_rev1.csv for each image. And X label is the image itself which is named by the image id and and stored in images_training_rev1 folder. 

YLabelCreate_augmented.py - This file reads the y label that is the 37 length vector from the csv and creates a list of image ids and dictionary with image id as key and 37 length vector as value. It further divides it into validation and training set. It does similar thing for augmentation, along with duplicating values twice for each image. 

data_loader.py - This file used PyTorch Custom Dataset to create the dataset. The images are read and transformed and returned as X and 37 lenth vectors are converted to torch tensor and returned as y. The dataset is further initialized using PyTorch DataLoader. The images are originally 424 x 424 color images. These images are first center cropped to 256 x 256 and then resized to 64 x 64. 

# Data Augmentation 

To increase the size of the dataset image augmentation is used. Following data transformations have been applied to each image twice - 
1. Rotation - random with angle between -90 to 90 degrees for first augmented image and -60 to 60 degree for second one. 
2. Translation - random with shift between -6 and 6 pixels for first augmented image and -12 to 12 pixels for second one.
3. Shear - random shear between 2 and -2 pixels for first augmented image and -3 to 3 pixels for second one.

The original data set had 61578 galaxies in training set. After augmentation we had three times the training set that is 184734 galaxies

# Models
The model used had following configuration - 
Layer 1 - Convolution Layer with input as 3 channel 64 x 64 image and output as 16 channel 60 x 60 image using a 5 x 5 square filter. ReLu nonliniearity was applied. Drop out with 0.1 probability was applied. 

Layer 2 - Convolution Layer with input as 16 channel, 60 x 60  and output as 32 channel, 56 x 56 using a 5 x 5 square filter. ReLu nonliniearity was applied. Drop out with 0.15 probability was applied. Further Maxpooling was also applied after this layer. 

Layer 3 - Convolution Layer with input as 32 channel, 28 x 28  and output as 64 channel, 26 x 26 using a 3 x 3 square filter. ReLu nonliniearity was applied. Drop out with 0.2 probability was applied. Further Maxpooling was also applied after this layer. 

Layer 4 - Convolution Layer with input as 64 channel, 13 x 13  and output as 128 channel, 11 x 11 using a 3 x 3 square filter. ReLu nonliniearity was applied. Drop out with 0.25 probability was applied. Further Maxpooling was also applied after this layer. 

Batch Normalization is used in all convolution layers. 

Layer 5 - Dense layer whose input is 128 x 5 x 5 flattened vector and output is 1024. Dropout of 0.3 probability is applied. 

Model_All_Questions.py 
